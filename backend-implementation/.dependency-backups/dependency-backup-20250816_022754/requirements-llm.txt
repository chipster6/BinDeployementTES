# ============================================================================
# LOCAL LLM SERVICE PYTHON REQUIREMENTS
# ============================================================================
#
# Production-ready Python dependencies for local LLM inference
# Optimized for Llama 3.1 8B with GPU acceleration
#
# Created by: DevOps-Agent (MESH Coordination coord-ai-ml-mesh-001)
# Date: 2025-08-13
# Version: 1.0.0
# ============================================================================

# Core ML/AI Libraries
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0
transformers==4.35.0
accelerate==0.24.0
bitsandbytes==0.41.1

# Tokenization and Model Handling
sentencepiece==0.1.99
tokenizers==0.14.1
safetensors==0.4.0
protobuf==4.24.4

# HuggingFace Integration
huggingface-hub==0.17.3
datasets==2.14.5

# Web Framework for LLM API
fastapi==0.103.0
uvicorn[standard]==0.23.2
pydantic==2.4.2
python-multipart==0.0.6
sse-starlette==1.6.5

# System Monitoring
psutil==5.9.5
GPUtil==1.4.0
nvidia-ml-py3==7.352.0

# Database and Cache
redis==4.6.0
hiredis==2.2.3
sqlalchemy==2.0.19

# HTTP and API
httpx==0.25.0
aiohttp==3.8.5
requests==2.31.0

# Configuration and Environment
python-dotenv==1.0.0
PyYAML==6.0.1
toml==0.10.2

# Monitoring and Logging
prometheus-client==0.17.1
structlog==23.1.0
sentry-sdk==1.32.0

# Performance Optimization
numpy==1.24.3
einops==0.7.0

# Security
cryptography==41.0.4
python-jose==3.3.0

# Utilities
tqdm==4.66.1
click==8.1.7
rich==13.5.2

# Testing
pytest==7.4.0
pytest-asyncio==0.21.1
pytest-cov==4.1.0

# Memory Management
pympler==0.9
memory-profiler==0.61.0

# Text Processing
nltk==3.8.1
spacy==3.7.0

# Development (optional)
ipython==8.14.0